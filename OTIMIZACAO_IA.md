# OtimizaÃ§Ã£o de Prompts para IA - SmartDoctor

## ğŸ“Š Problema Identificado

O sistema estava enviando um prompt de **~150 linhas** a cada interaÃ§Ã£o com a IA, aumentando:
- **Custo**: Mais tokens = mais dinheiro
- **LatÃªncia**: Mais dados para processar = resposta mais lenta
- **Limites**: Maior chance de exceder limites de contexto

## âœ… SoluÃ§Ã£o Implementada: OtimizaÃ§Ã£o do Prompt

### ReduÃ§Ã£o Aplicada
- **Antes**: ~150 linhas de instruÃ§Ãµes detalhadas
- **Depois**: ~15 linhas de instruÃ§Ãµes essenciais
- **ReduÃ§Ã£o**: ~70% do tamanho do prompt

### EstratÃ©gia
1. **ConsolidaÃ§Ã£o**: Removidas redundÃ¢ncias e repetiÃ§Ãµes
2. **MovimentaÃ§Ã£o**: InstruÃ§Ãµes detalhadas movidas para as `descriptions` das funÃ§Ãµes
3. **SimplificaÃ§Ã£o**: Mantidas apenas regras crÃ­ticas no system message

## ğŸš€ Outras EstratÃ©gias DisponÃ­veis

### 1. Fine-Tuning (Treinar Modelo Customizado)

#### O que Ã©?
Treinar um modelo com suas instruÃ§Ãµes e exemplos de conversas, criando um modelo especializado.

#### Vantagens
- âœ… Prompt mÃ­nimo (apenas contexto dinÃ¢mico)
- âœ… Comportamento mais consistente
- âœ… Menor custo por token apÃ³s treinamento
- âœ… Respostas mais rÃ¡pidas

#### Desvantagens
- âŒ Custo inicial de treinamento (~$8-50 por 1M tokens)
- âŒ Tempo de treinamento (algumas horas)
- âŒ AtualizaÃ§Ãµes requerem retreino
- âŒ Precisa de dataset de exemplos

#### Quando Usar
- Comportamento do assistente Ã© estÃ¡vel
- VocÃª tem muitas conversas de exemplo
- Quer reduzir custos a longo prazo
- Precisa de comportamento muito especÃ­fico

#### Como Implementar

```typescript
// 1. Preparar dataset de treinamento (formato JSONL)
const trainingData = [
  {
    messages: [
      { role: "system", content: "VocÃª Ã© um assistente..." },
      { role: "user", content: "O que tem pra hoje?" },
      { role: "assistant", content: "Hoje vocÃª tem..." }
    ]
  },
  // ... mais exemplos
];

// 2. Upload para OpenAI
const file = await openai.files.create({
  file: fs.createReadStream("training_data.jsonl"),
  purpose: "fine-tune"
});

// 3. Criar job de fine-tuning
const fineTune = await openai.fineTuning.jobs.create({
  training_file: file.id,
  model: "gpt-4o-mini" // ou gpt-3.5-turbo
});

// 4. Usar modelo fine-tuned
const response = await openai.chat.completions.create({
  model: "ft:gpt-4o-mini:org:custom-name:abc123", // ID do modelo treinado
  messages: [
    { role: "system", content: "Data atual: 2025-11-26" }, // Apenas contexto dinÃ¢mico
    ...userMessages
  ]
});
```

#### Custo Estimado
- **Treinamento**: ~$8-50 por 1M tokens
- **Uso**: Mesmo custo do modelo base
- **ROI**: Economia apÃ³s ~10k-50k mensagens

---

### 2. Embeddings + RAG (Retrieval Augmented Generation)

#### O que Ã©?
Armazenar instruÃ§Ãµes como embeddings e buscar apenas o contexto relevante por consulta.

#### Vantagens
- âœ… Prompt dinÃ¢mico baseado na pergunta
- âœ… EscalÃ¡vel para muitas instruÃ§Ãµes
- âœ… Pode incluir documentaÃ§Ã£o completa
- âœ… NÃ£o requer treinamento

#### Desvantagens
- âŒ Complexidade adicional
- âŒ Custo de embeddings
- âŒ Pode perder contexto se busca falhar

#### Quando Usar
- Muitas regras/instruÃ§Ãµes diferentes
- InstruÃ§Ãµes mudam frequentemente
- Quer incluir documentaÃ§Ã£o completa

#### Como Implementar

```typescript
import { OpenAIEmbeddings } from "@langchain/openai";
import { VectorStore } from "langchain/vectorstores";

// 1. Criar embeddings das instruÃ§Ãµes
const embeddings = new OpenAIEmbeddings();
const instructionChunks = [
  "Regra sobre datas: Use hoje para...",
  "Regra sobre agendamentos: Formato...",
  // ... todas as instruÃ§Ãµes
];

// 2. Armazenar em vector store
const vectorStore = await VectorStore.fromTexts(
  instructionChunks,
  [{ type: "instruction" }],
  embeddings
);

// 3. Buscar contexto relevante
async function getRelevantContext(userQuery: string) {
  const results = await vectorStore.similaritySearch(userQuery, 3);
  return results.map(r => r.pageContent).join("\n\n");
}

// 4. Usar no prompt
const context = await getRelevantContext(lastUserMessage);
const systemMessage = `VocÃª Ã© um assistente. Contexto relevante:\n${context}`;
```

---

### 3. Modelos Menores com Fine-Tuning

#### O que Ã©?
Fine-tuning de modelos menores e mais baratos (GPT-3.5-turbo, GPT-4o-mini).

#### Vantagens
- âœ… Custo muito menor
- âœ… Respostas mais rÃ¡pidas
- âœ… MantÃ©m qualidade com fine-tuning

#### Desvantagens
- âŒ Capacidade limitada vs GPT-4
- âŒ Pode precisar de mais exemplos

#### Quando Usar
- Tarefas especÃ­ficas e bem definidas
- OrÃ§amento limitado
- Precisa de baixa latÃªncia

#### Custo Comparativo
- **GPT-4o**: $2.50/$10 por 1M tokens (input/output)
- **GPT-4o-mini**: $0.15/$0.60 por 1M tokens
- **GPT-3.5-turbo**: $0.50/$1.50 por 1M tokens

---

### 4. EstratÃ©gia HÃ­brida (Recomendada)

#### CombinaÃ§Ã£o
1. **Prompt base mÃ­nimo** (jÃ¡ implementado)
2. **Fine-tuning** para comportamento especÃ­fico
3. **Contexto dinÃ¢mico** apenas quando necessÃ¡rio

#### ImplementaÃ§Ã£o

```typescript
// Prompt base mÃ­nimo (sempre)
const baseSystemMessage = `VocÃª Ã© um assistente para agendamentos.`;

// Contexto dinÃ¢mico (apenas quando necessÃ¡rio)
function getDynamicContext(userQuery: string, today: string) {
  const context: string[] = [];
  
  // Adicionar apenas contexto relevante
  if (userQuery.includes("hoje") || userQuery.includes("amanhÃ£")) {
    context.push(`Data atual: ${today}`);
  }
  
  if (userQuery.includes("valor") || userQuery.includes("recebi")) {
    context.push("Formato valores: R$ X.XXX,XX");
  }
  
  return context.join("\n");
}

const systemMessage = `${baseSystemMessage}\n${getDynamicContext(lastUserMessage, todayStr)}`;
```

---

## ğŸ“ˆ ComparaÃ§Ã£o de EstratÃ©gias

| EstratÃ©gia | ReduÃ§Ã£o de Tokens | Custo Inicial | Complexidade | Melhor Para |
|------------|-------------------|---------------|--------------|-------------|
| **OtimizaÃ§Ã£o de Prompt** | 70% | $0 | Baixa | âœ… Implementado |
| **Fine-Tuning** | 90% | $50-500 | MÃ©dia | Longo prazo |
| **RAG** | 60-80% | $0-100 | Alta | Muitas regras |
| **Modelo Menor** | 70% + custo menor | $50-500 | MÃ©dia | OrÃ§amento limitado |
| **HÃ­brida** | 85% | $50-500 | MÃ©dia-Alta | â­ Recomendado |

---

## ğŸ¯ PrÃ³ximos Passos Recomendados

### Fase 1: Monitorar (Atual)
- âœ… OtimizaÃ§Ã£o de prompt implementada
- ğŸ“Š Monitorar custos e latÃªncia
- ğŸ“ Coletar exemplos de conversas

### Fase 2: Fine-Tuning (1-2 meses)
- Coletar 500-1000 exemplos de conversas
- Treinar modelo GPT-4o-mini
- Testar e comparar resultados

### Fase 3: OtimizaÃ§Ã£o AvanÃ§ada (3-6 meses)
- Implementar contexto dinÃ¢mico
- Considerar RAG se necessÃ¡rio
- Ajustar baseado em mÃ©tricas

---

## ğŸ“ Notas TÃ©cnicas

### Tokens vs Linhas
- 1 linha â‰ˆ 10-20 tokens
- Prompt antigo: ~2000-3000 tokens
- Prompt novo: ~200-400 tokens
- **Economia**: ~$0.004-0.006 por chamada (GPT-4o)

### Limites de Contexto
- GPT-4o: 128k tokens
- GPT-4o-mini: 128k tokens
- Com prompt otimizado, sobra mais espaÃ§o para histÃ³rico

### Custo Estimado
- **Antes**: ~$0.006-0.009 por chamada
- **Depois**: ~$0.001-0.002 por chamada
- **Economia**: ~70% por chamada

---

## ğŸ”— ReferÃªncias

- [OpenAI Fine-Tuning Guide](https://platform.openai.com/docs/guides/fine-tuning)
- [OpenAI Pricing](https://openai.com/api/pricing/)
- [LangChain RAG](https://js.langchain.com/docs/use_cases/question_answering/)







